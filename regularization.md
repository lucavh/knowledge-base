Regularization is a fundamental concept in [[machine-learning]] used to prevent overfitting and enhance the [[generalization]] ability of models. It introduces additional constraints or penalties to the learning process, encouraging simpler and more stable models.

### Importance of Regularization

- **Overfitting Mitigation**: Overfitting occurs when a model captures noise in the training data, leading to poor performance on new data. Regularization helps to counter this by imposing limitations on the complexity of the model.
- **[[generalization]] Enhancement**: Regularized models are more likely to generalize well on unseen data, as they prioritize capturing essential patterns rather than memorizing the training data.

### Common Regularization Techniques

- **L1 Regularization (Lasso)**: Adds a penalty proportional to the absolute value of the model's coefficients, leading to sparsity (some coefficients become exactly zero), effectively performing feature selection.
- **L2 Regularization (Ridge)**: Introduces a penalty proportional to the square of the model's coefficients. It discourages large coefficient values, promoting smoother and more stable models.
- **Elastic Net Regularization**: A combination of L1 and L2 regularization, providing a balance between feature selection (L1) and coefficient shrinkage (L2).

Both L1 and L2 regularization prevent overfitting by shrinking (imposing a penalty) on the coefficients. With L2, you tend to end up with many small weights, while with L1, you tend to end up with larger weights, but more zeros.

### Regularization in Different Models

- **[[linear-regression]]**: Regularized versions are Ridge [[Regression]] (L2 regularization) and Lasso [[Regression]] (L1 regularization).
- **[[logistic-regression]]**: Regularization can help prevent overfitting, especially when dealing with high-dimensional data.
- **[[neural-networks]]**: Techniques like dropout and weight decay introduce regularization by randomly deactivating neurons during training and penalizing large weight values, respectively.
    
### Hyperparameter Tuning

Regularization introduces hyperparameters (e.g., regularization strength) that need to be tuned. [[Cross-validation]] and grid search are common methods to find optimal hyperparameter values.

### Balancing Act

- **Under-regularization**: Leads to overfitting and poor [[generalization]].
- **Over-regularization**: Hampers the model's ability to capture important patterns.

Regularization is a cornerstone of effective machine learning, striking a balance between complexity and simplicity, and facilitating models that perform well on both training and new data.