In ranking tasks, where the goal is to predict the order or ranking of items, evaluation metrics are tailored to assess the quality of the ranked lists.

- ****Normalized Discounted Cumulative Gain (NDCG)****

NDCG is a widely used metric for ranking tasks. It measures the quality of a ranked list by considering the relevance of items and their positions in the list. It’s especially common in recommendation systems.

- ****Precision at K****

Precision at K measures the proportion of relevant items in the top K positions of the ranked list. It’s used to evaluate how well a model ranks relevant items at the top.

- ****Mean Reciprocal Rank (MRR)****

MRR calculates the average reciprocal rank of the first relevant item in the ranked list. It provides a single value that summarizes the model’s ability to rank relevant items highly.

[[model-evaluation]]