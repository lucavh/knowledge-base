{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Scikit-learn glossary](https://scikit-learn.org/stable/glossary.html)\n",
    "- [Scikit-learn MOOC](https://inria.github.io/scikit-learn-mooc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipilines and model diagrams with `make_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 {color: black;background-color: white;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 pre{padding: 0;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-toggleable {background-color: white;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-estimator:hover {background-color: #d4ebff;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-item {z-index: 1;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-parallel-item:only-child::after {width: 0;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-ad8cf440-87a0-4a9e-a6c5-c171cb6a4f87\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b140c374-56f2-45a0-966d-c7f610412e47\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b140c374-56f2-45a0-966d-c7f610412e47\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e546e4e2-b489-4061-a6d0-1145934874c7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e546e4e2-b489-4061-a6d0-1145934874c7\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c9ef9b1d-5b4e-418e-b38c-bc0f3b6bbcd5\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c9ef9b1d-5b4e-418e-b38c-bc0f3b6bbcd5\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# to display nice model diagram\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Preprocessing pipeline with `ColumnTransformer`\n",
    "\n",
    "Use pipelines to write efficient code and reduce the risk of data leakage and encode multiple categorical variables at once with `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('standard_scaler', numerical_preprocessor, numerical_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Select features based on their column type with `make_column_selector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of categorical variables\n",
    "\n",
    "#### Ordinal categories with `OrdinalEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[\"S\", \"M\", \"L\", \"XL\"])\n",
    "data_encoded = encoder.fit_transform(data[[\"size\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal categories with `OneHotEncoder` (without assuming any order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "data_encoded = encoder.fit_transform(data_categorical)\n",
    "\n",
    "columns_encoded = encoder.get_feature_names_out(data_categorical.columns)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns=columns_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset with `train_test_split` (and example of using pipelines)\n",
    "\n",
    "Use `stratify` attrbute for inbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.934\n",
      "MAE: 144.944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Load data\n",
    "cars = (\n",
    "    pd.read_csv(\"../data/cars_dataset.csv\")\n",
    "    .rename(str.lower, axis=\"columns\")\n",
    "    .loc[lambda df: df[\"cylinders\"] > 3]\n",
    "    .dropna()\n",
    "    .assign(year=lambda df: df[\"year\"].str.slice(0,4).astype('int'))\n",
    ")\n",
    "\n",
    "feature_columns = [\"miles_per_gallon\", \"cylinders\", \"displacement\", \"horsepower\", \"acceleration\", \"year\", \"origin\"]\n",
    "\n",
    "X = cars[feature_columns]\n",
    "y = cars[\"weight_in_lbs\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)\n",
    "\n",
    "# Define model\n",
    "categorical_columns = [\"origin\"]\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"error\"), categorical_columns)\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "model = Pipeline(steps=[\n",
    "      (\"encoding\", ct),\n",
    "      (\"scaler\", StandardScaler()),\n",
    "      (\"estimator\", GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred  = model.predict(X_test)\n",
    "\n",
    "# Evaluate results\n",
    "r2 = r2_score(y_test, y_pred).round(3)\n",
    "mae = mean_absolute_error(y_test, y_pred).round(3)\n",
    "\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom classes\n",
    "\n",
    "### Template for custom transformer with `BaseEstimator` and `TransformerMixin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, my_arg):\n",
    "        # super().__init__() --> use __init__() of parent class, in this case not needed as BaseEstimator does not have init method\n",
    "        self.my_arg = my_arg\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.my_param_ = ... # trailing _ to indicate that parameter is set during fitting/training\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = ...\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template for custom estimator with `BaseEstimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class CustomEstimator(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, my_arg):\n",
    "        # super().__init__() --> use __init__() of parent class, in this case not needed as BaseEstimator does not have init method\n",
    "        self.my_arg = my_arg\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        estimator = ...\n",
    "        self.estimator_ = estimator.fit(X, y=y)  # trailing _ to indicate that parameter is set during fitting/training\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, **predict_params):\n",
    "        check_is_fitted(self, [\"estimator_\"])\n",
    "        return self.estimator_.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "### Cross validation with `cross_val_score` and `cross_validate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Basic cv\n",
    "cv = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# Bootstrapping cv\n",
    "cv = ShuffleSplit(n_splits=2,  test_size=0.2, random_state=0)\n",
    "\n",
    "test_scores = cross_val_score(model, data, target, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X, y)\n",
    "\n",
    "scores = cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **nested cross-validation**, we use an inner\n",
    "cross-validation for the selection of the hyperparameters and an outer\n",
    "cross-validation for the evaluation of generalization performance of the\n",
    "refitted tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('cat_preprocessor', categorical_preprocessor, categorical_columns),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    sparse_threshold=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\n",
    "        \"classifier\",\n",
    "        HistGradientBoostingClassifier(\n",
    "            random_state=42, max_leaf_nodes=4\n",
    "        )\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define hyperparameter tuning with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': (0.05, 0.5),\n",
    "    'classifier__max_leaf_nodes': (10, 30),\n",
    "}\n",
    "model_grid_search = GridSearchCV(\n",
    "    model, param_grid=param_grid, n_jobs=2, cv=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Perform netsted cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    model_grid_search, data, target, cv=5, n_jobs=2, return_estimator=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Check model generalization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_test_scores = cv_results['test_score']\n",
    "print(\n",
    "    \"Generalization score with hyperparameters tuning:\\n\"\n",
    "    f\"{cv_test_scores.mean():.3f} +/- {cv_test_scores.std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. In addition, passing the parameter `return_estimator=True`, we can check the\n",
    "value of the best hyperparameters obtained for each fold of the outer\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv_fold, estimator_in_fold in enumerate(cv_results[\"estimator\"]):\n",
    "    print(\n",
    "        f\"Best hyperparameters for fold #{cv_fold + 1}:\\n\"\n",
    "        f\"{estimator_in_fold.best_params_}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a schematic representation of the complete nested cross-validation\n",
    "procedure:\n",
    "\n",
    "![Nested cross-validation\n",
    "diagram](../images/nested_cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model parameters (scikit-learn convention): if an attribute is learned from the data, its name ends with an underscore (i.e. `_`), as in `mean_` and `scale_` for the `StandardScaler`.\n",
    "- [scikit-lego](https://scikit-lego.readthedocs.io/en/latest/) repository for custom models not in scikit-learn by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
